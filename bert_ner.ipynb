{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Falk358/Bert_NER/blob/main/bert_ner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT Named Entity Recognition\n",
        "this colab notebook was strongly inspired by this tutorial: https://towardsdatascience.com/named-entity-recognition-with-bert-in-pytorch-a454405e0b6a\n"
      ],
      "metadata": {
        "id": "vQOZ3X64ybhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIWTWNfizJfp",
        "outputId": "c8effe8b-a9ff-45b8-ff2f-28794cae6bc6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas import DataFrame as df\n",
        "from pandas import read_csv\n",
        "from transformers import BertTokenizerFast\n",
        "from transformers.tokenization_utils_base import BatchEncoding\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "yaVQeX4Ty1qL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, lets start by loading the dataset uploaded to our github repo:\n",
        "\n",
        "the dataset is for Named Entitity recognition and provides the following labels:\n",
        "\n",
        "1. `geo` for geographical entity\n",
        "2. `org` for organization entity\n",
        "3. `per` for person entity\n",
        "4. `gpe` for geopolitical entity\n",
        "5. `tim` for time indicator entity\n",
        "6. `art` for artifact entity\n",
        "7. `eve` for event entity\n",
        "8. `nat` for natural phenomenon entity\n",
        "9. `0` if the word doesn't belong to any above label\n"
      ],
      "metadata": {
        "id": "eI2p04uaygsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_url = \"https://raw.githubusercontent.com/Falk358/Bert_NER/main/dataset/ner.csv\"\n",
        "\n",
        "dataset_df = read_csv(dataset_url)\n",
        "dataset_df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "8VenFb28yui6",
        "outputId": "7299bdbb-aab9-475b-d3d6-9198f97d1ab1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  \\\n",
              "0  Thousands of demonstrators have marched throug...   \n",
              "1  Iranian officials say they expect to get acces...   \n",
              "2  Helicopter gunships Saturday pounded militant ...   \n",
              "3  They left after a tense hour-long standoff wit...   \n",
              "4  U.N. relief coordinator Jan Egeland said Sunda...   \n",
              "\n",
              "                                              labels  \n",
              "0  O O O O O O B-geo O O O O O B-geo O O O O O B-...  \n",
              "1  B-gpe O O O O O O O O O O O O O O B-tim O O O ...  \n",
              "2  O O B-tim O O O O O B-geo O O O O O B-org O O ...  \n",
              "3                              O O O O O O O O O O O  \n",
              "4  B-geo O O B-per I-per O B-tim O B-geo O B-gpe ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d5d9bebc-fe1c-45bb-a06f-7b6ca19e7982\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Thousands of demonstrators have marched throug...</td>\n",
              "      <td>O O O O O O B-geo O O O O O B-geo O O O O O B-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Iranian officials say they expect to get acces...</td>\n",
              "      <td>B-gpe O O O O O O O O O O O O O O B-tim O O O ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Helicopter gunships Saturday pounded militant ...</td>\n",
              "      <td>O O B-tim O O O O O B-geo O O O O O B-org O O ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>They left after a tense hour-long standoff wit...</td>\n",
              "      <td>O O O O O O O O O O O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>U.N. relief coordinator Jan Egeland said Sunda...</td>\n",
              "      <td>B-geo O O B-per I-per O B-tim O B-geo O B-gpe ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d5d9bebc-fe1c-45bb-a06f-7b6ca19e7982')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d5d9bebc-fe1c-45bb-a06f-7b6ca19e7982 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d5d9bebc-fe1c-45bb-a06f-7b6ca19e7982');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing and tokenization\n",
        "\n",
        "We need to tokenize our dataset. However, tokenization splits some words into multiple parts, so we need to adjust our labels so that they still match correctly. Furthermore, we will define the `Dataset` class for `pytorch`"
      ],
      "metadata": {
        "id": "UxHe-5zQ1dIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "text_list = dataset_df[\"text\"].values.tolist()\n",
        "\n",
        "print(text_list[0])\n",
        "\n",
        "text_tokenized_example = tokenizer(text_list[0], padding='max_length', max_length=512, truncation=True, return_tensors=\"pt\") # use padding of 512 (needed for BERT) and pytorch tensor format (pt)\n",
        "print(type(text_tokenized_example))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPV3JBTp2BBL",
        "outputId": "70697e84-918d-44a5-c73d-9f469c25b8e0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .\n",
            "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def extract_unique_labels(dataset: df) -> set:\n",
        "  \n",
        "  # Split labels based on whitespace and turn them into a list\n",
        "  labels = [i.split() for i in dataset['labels'].values.tolist()]\n",
        "\n",
        "  # Check how many labels are there in the dataset\n",
        "  unique_labels = set()\n",
        "\n",
        "  for lb in labels:\n",
        "    [unique_labels.add(i) for i in lb if i not in unique_labels]\n",
        "  \n",
        "  return unique_labels\n",
        "\n",
        "\n",
        "def map_labels_to_ids(dataset: df) -> dict:\n",
        "  \n",
        "  unique_labels = extract_unique_labels(dataset)\n",
        " \n",
        "  # Map each label into its id representation and vice versa\n",
        "  labels_to_ids = {k: v for v, k in enumerate(sorted(unique_labels))}\n",
        "  return labels_to_ids\n",
        "\n",
        "\n",
        "def map_ids_to_labels(dataset: df) -> dict:\n",
        "  \n",
        "  unique_labels = extract_unique_labels(dataset)\n",
        "\n",
        "  # Map each label into its id representation and vice versa\n",
        "  ids_to_labels = {v: k for v, k in enumerate(sorted(unique_labels))}\n",
        "  return ids_to_labels\n",
        "\n",
        "\n",
        "def align_label_sentence(tokenized_sentence: BatchEncoding, labels: str, labels_to_ids: dict)-> list: # aligns labels for a single sentence (row in dataframe)\n",
        "  word_ids = tokenized_sentence.word_ids()\n",
        "\n",
        "  previous_word_idx = None\n",
        "  label_ids = []\n",
        "   \n",
        "  for word_idx in word_ids: # word_ids is target length for labels vector\n",
        "\n",
        "    if word_idx is None:\n",
        "      label_ids.append(-100)\n",
        "                \n",
        "    elif word_idx != previous_word_idx: # \n",
        "        try:\n",
        "          label_ids.append(labels_to_ids[labels[word_idx]])\n",
        "        except:\n",
        "          label_ids.append(-100)\n",
        "        \n",
        "    else:\n",
        "      label_ids.append(-100) # only the first token of a word will be labelled, the rest get -100\n",
        "      previous_word_idx = word_idx\n",
        "      \n",
        "\n",
        "  return label_ids"
      ],
      "metadata": {
        "id": "lfaqexYi4_vQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataSequence(torch.utils.data.Dataset): # defines pytorch dataset\n",
        "\n",
        "  def __init__(self, datset_df: df, labels_to_ids: dict):\n",
        "\n",
        "    lb = [i.split() for i in dataset_df['labels'].values.tolist()]\n",
        "    txt = dataset_df['text'].values.tolist()\n",
        "    self.texts = [tokenizer(str(i), padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\") for i in txt]\n",
        "    self.labels = [align_label_sentence(i,j, labels_to_ids) for i,j in zip(self.texts, lb)]\n",
        "\n",
        "  def __len__(self):\n",
        "\n",
        "    return len(self.labels)\n",
        "\n",
        "  def get_batch_data(self, idx):\n",
        "\n",
        "    return self.texts[idx]\n",
        "\n",
        "  def get_batch_labels(self, idx):\n",
        "\n",
        "    return torch.LongTensor(self.labels[idx])\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    batch_data = self.get_batch_data(idx)\n",
        "    batch_labels = self.get_batch_labels(idx)\n",
        "\n",
        "    return batch_data, batch_labels"
      ],
      "metadata": {
        "id": "dSr5TwDIArDp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT Model definition\n",
        "\n",
        "Here, we define the class for the `BERT` model itself."
      ],
      "metadata": {
        "id": "-mNaNFXfCav7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForTokenClassification\n",
        "\n",
        "class BertModel(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, unique_labels: set):\n",
        "\n",
        "    super(BertModel, self).__init__()\n",
        "\n",
        "    self.bert = BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=len(unique_labels))\n",
        "\n",
        "  def forward(self, input_id, mask, label):\n",
        "\n",
        "    output = self.bert(input_ids=input_id, attention_mask=mask, labels=label, return_dict=False)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "WFUB0UlPC0IQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "\n",
        "Here, we train the model by defining the training loop function:"
      ],
      "metadata": {
        "id": "jKH7CKAvFU7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import SGD\n",
        "\n",
        "def train_loop(model, df_train: df, df_val: df, labels_to_ids: dict, LEARNING_RATE: float, BATCH_SIZE: int, EPOCHS: int):\n",
        "\n",
        "  train_dataset = DataSequence(df_train, labels_to_ids)\n",
        "  val_dataset = DataSequence(df_val, labels_to_ids)\n",
        "\n",
        "  \n",
        "  train_dataloader = DataLoader(train_dataset, num_workers=4, batch_size=BATCH_SIZE, shuffle=True)\n",
        "  val_dataloader = DataLoader(val_dataset, num_workers=4, batch_size=BATCH_SIZE)\n",
        "\n",
        "  use_cuda = torch.cuda.is_available()\n",
        "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "  print(f\"device used: {device}\")\n",
        "  optimizer = SGD(model.parameters(), lr=LEARNING_RATE) # use stochastic gradient descent\n",
        "\n",
        "  if use_cuda:\n",
        "      model = model.cuda()\n",
        "\n",
        "  best_acc = 0\n",
        "  best_loss = 1000\n",
        "\n",
        "  for epoch_num in range(EPOCHS):\n",
        "\n",
        "      total_acc_train = 0\n",
        "      total_loss_train = 0\n",
        "\n",
        "      model.train()\n",
        "\n",
        "      for train_data, train_label in tqdm(train_dataloader):\n",
        "\n",
        "          train_label = train_label.to(device)\n",
        "          mask = train_data['attention_mask'].squeeze(1).to(device)\n",
        "          input_id = train_data['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          loss, logits = model(input_id, mask, train_label)\n",
        "\n",
        "          for i in range(logits.shape[0]):\n",
        "\n",
        "            logits_clean = logits[i][train_label[i] != -100]\n",
        "            label_clean = train_label[i][train_label[i] != -100]\n",
        "\n",
        "            predictions = logits_clean.argmax(dim=1)\n",
        "            acc = (predictions == label_clean).float().mean()\n",
        "            total_acc_train += acc\n",
        "            total_loss_train += loss.item()\n",
        "\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "      model.eval()\n",
        "\n",
        "      total_acc_val = 0\n",
        "      total_loss_val = 0\n",
        "\n",
        "      for val_data, val_label in val_dataloader:\n",
        "\n",
        "          val_label = val_label.to(device)\n",
        "          mask = val_data['attention_mask'].squeeze(1).to(device)\n",
        "          input_id = val_data['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "          loss, logits = model(input_id, mask, val_label)\n",
        "\n",
        "          for i in range(logits.shape[0]):\n",
        "\n",
        "            logits_clean = logits[i][val_label[i] != -100]\n",
        "            label_clean = val_label[i][val_label[i] != -100]\n",
        "\n",
        "            predictions = logits_clean.argmax(dim=1)\n",
        "            acc = (predictions == label_clean).float().mean()\n",
        "            total_acc_val += acc\n",
        "            total_loss_val += loss.item()\n",
        "\n",
        "      val_accuracy = total_acc_val / len(df_val)\n",
        "      val_loss = total_loss_val / len(df_val)\n",
        "\n",
        "      print(f'Epochs: {epoch_num + 1} | Loss: {total_loss_train / len(df_train): .3f} | Accuracy: {total_acc_train / len(df_train): .3f} | Val_Loss: {total_loss_val / len(df_val): .3f} | Accuracy: {total_acc_val / len(df_val): .3f}')\n",
        "\n",
        "\n",
        "# EVERYTHING RUNS HERE\n",
        "LEARNING_RATE = 5e-3\n",
        "EPOCHS = 5\n",
        "BATCH_SIZE = 10\n",
        "\n",
        "dataset_df = dataset_df[0:2000] # only use 1000 entries (long loading times for dataset)\n",
        "df_train, df_val, df_test = np.split(dataset_df.sample(frac=1, random_state=42), [int(.8 * len(dataset_df)), int(.9 * len(dataset_df))])\n",
        "\n",
        "unique_labels = extract_unique_labels(dataset_df)\n",
        "labels_to_ids = map_labels_to_ids(dataset_df)\n",
        "model = BertModel(unique_labels)\n",
        "train_loop(model, df_train, df_val, labels_to_ids, LEARNING_RATE, BATCH_SIZE, EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bekTV5GjFbEz",
        "outputId": "429a62db-a28d-4d55-df65-b5232ee7d37e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device used: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [02:45<00:00,  1.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 1 | Loss:  0.952 | Accuracy:  1.039 | Val_Loss:  6.120 | Accuracy:  8.413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  8%|▊         | 17/200 [00:14<02:31,  1.21it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "d-cyagkym0bC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, df_test: df, labels_to_ids: dict):\n",
        "  \n",
        "  test_dataset = DataSequence(df_test, labels_to_ids)\n",
        "\n",
        "  test_dataloader = DataLoader(test_dataset, num_workers=4, batch_size=1)\n",
        "\n",
        "  use_cuda = torch.cuda.is_available()\n",
        "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "  if use_cuda:\n",
        "      model = model.cuda()\n",
        "\n",
        "  total_acc_test = 0.0\n",
        "\n",
        "  for test_data, test_label in test_dataloader:\n",
        "\n",
        "          test_label = test_label.to(device)\n",
        "          mask = test_data['attention_mask'].squeeze(1).to(device)\n",
        "\n",
        "          input_id = test_data['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "          loss, logits = model(input_id, mask, test_label)\n",
        "\n",
        "          for i in range(logits.shape[0]):\n",
        "\n",
        "            logits_clean = logits[i][test_label[i] != -100]\n",
        "            label_clean = test_label[i][test_label[i] != -100]\n",
        "\n",
        "            predictions = logits_clean.argmax(dim=1)\n",
        "            acc = (predictions == label_clean).float().mean()\n",
        "            total_acc_test += acc\n",
        "\n",
        "  val_accuracy = total_acc_test / len(df_test)\n",
        "  print(f'Test Accuracy: {total_acc_test / len(df_test): .3f}')\n",
        "\n",
        "\n",
        "evaluate(model, df_test, labels_to_ids)"
      ],
      "metadata": {
        "id": "hNdwCcszeXAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run inference on your own text\n",
        "\n",
        "The code in this section accepts a string as input and uses the fine tuned bert model we just created to perform Named Entity Recognition"
      ],
      "metadata": {
        "id": "O1GpMN3onVVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_align_ids(text: str) -> list:\n",
        "  tokenized_inputs = tokenizer(text, padding='max_length', max_length=512, truncation=True)\n",
        "\n",
        "  word_ids = tokenized_inputs.word_ids()\n",
        "\n",
        "  previous_word_idx = None\n",
        "  label_ids = []\n",
        "\n",
        "  for word_idx in word_ids:\n",
        "\n",
        "      if word_idx is None:\n",
        "        label_ids.append(-100)\n",
        "\n",
        "      elif word_idx != previous_word_idx:\n",
        "          try:\n",
        "            label_ids.append(1)\n",
        "          except:\n",
        "            label_ids.append(-100)\n",
        "      else:\n",
        "          try:\n",
        "            label_ids.append(-100)\n",
        "          except:\n",
        "            label_ids.append(-100)\n",
        "      previous_word_idx = word_idx\n",
        "\n",
        "  return label_ids\n",
        "\n",
        "def evaluate_one_text(model, ids_to_labels: dict, sentence):\n",
        "\n",
        "\n",
        "  use_cuda = torch.cuda.is_available()\n",
        "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "  if use_cuda:\n",
        "    model = model.cuda()\n",
        "\n",
        "  text = tokenizer(sentence, padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "  mask = text['attention_mask'].to(device)\n",
        "  input_id = text['input_ids'].to(device)\n",
        "  label_ids = torch.Tensor(tokenize_and_align_ids(sentence)).unsqueeze(0).to(device)\n",
        "\n",
        "  logits = model(input_id, mask, None)\n",
        "  logits_clean = logits[0][label_ids != -100]\n",
        "\n",
        "  predictions = logits_clean.argmax(dim=1).tolist()\n",
        "  prediction_label = [ids_to_labels[i] for i in predictions]\n",
        "  print(sentence)\n",
        "  print(prediction_label)\n",
        "\n",
        "\n",
        "ids_to_labels = map_ids_to_labels(dataset_df)\n",
        "evaluate_one_text(model, ids_to_labels, 'Bill Gates is the founder of Microsoft')"
      ],
      "metadata": {
        "id": "NR0Zvy5Hn36N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}